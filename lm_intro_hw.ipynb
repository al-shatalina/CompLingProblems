{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def ngrammer(tokens, n=3):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488198d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "news = codecs.open( \"lenta.txt\", \"r\", \"utf_8_sig\" ).read() \n",
    "\n",
    "norm_news = normalize(news[:5000000])\n",
    "\n",
    "vocab_news = Counter(norm_news)\n",
    "\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4ce425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>','<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8798ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "threegrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence,2))\n",
    "    threegrams_news.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf8b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c22c53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                        len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bi_news = list(bigrams_news)\n",
    "bi2id_news = {bi:i for i, bi in enumerate(id2bi_news)}\n",
    "\n",
    "\n",
    "for ngram in threegrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + \" \" + word2\n",
    "    matrix_news[bi2id_news[bigram], word2id_news[word3]] =  (threegrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])\n",
    "    \n",
    "matrix_news = csc_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "99734064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_temperature(probas, temperature):\n",
    "    log_probas = np.log(np.maximum(probas, 1e-10))  \n",
    "    adjusted_log_probas = log_probas / temperature\n",
    "    exp_probas = np.exp(adjusted_log_probas)\n",
    "    adjusted_probabilities = exp_probas / np.sum(exp_probas)\n",
    "    return adjusted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "352ce46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bi2id, n=200, start='<start> <start>'):\n",
    "    text = [\"<start>\" , \"<start>\"]\n",
    "    current_idx = bi2id[start]\n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen_idx = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        chosen = id2word[chosen_idx]\n",
    "        text.append(chosen)\n",
    "\n",
    "        if chosen == '<end>':\n",
    "            current_idx = bi2id['<start> <start>']\n",
    "            text.extend(['<start>', '<start>'])\n",
    "        else:\n",
    "            current_idx = bi2id[\" \".join(text[-2:])]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9e0125d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " периодически бои вспыхивают на улицах столицы красовались плакаты в поддержку мэра москвы \n",
      "  это вызвано давлением со стороны старопромысловского старосунженского районов и самых дальних деревень \n",
      "  вешняков так и не научились заключил генерал лебедь \n",
      "  высланные из столицы люди не могли найти англичане при обыске банка следствие обнаружило свидетельства позволяющие подозревать банк фламинго в незаконных финансовых операциях с использованием в россии финансовых средств на банковских корсчетах продолжают оставаться на своем заседании 24 января ставку рефинансирования на 0,5 процента \n",
      "  30 декабря 1999 года на святой земле первый президент россии борис ельцин намерен посетить ботлихский район дагестана а также увеличения числа работающих в области безопасности проведение совместных военных маневров и консультаций на высоком уровне боевой готовности и отвечают самым современным требованиям \n",
      "  однако слушания были посвященытеме коррупции и отмывания денег российскими преступниками через каналы bank of new york и стремление обезопасить свидетелей \n",
      "  они крушили научное оборудование базы пытаясь таким образом впервые стали говорить о более чем на 4 процента \n",
      "  они не подумали об имидже россии \n",
      "  как сообщает итар-тасс сумма выкупа уже заплаченного двумя лондонскими финансовыми учреждениями и полицией и призывающая представителей крупного бизнеса районах \n",
      "  чубайс отметил что по поручению президента ингушетии сейчас на дорогах многие из них\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, bi2id_news).replace('<end>', '\\n').replace('<start> <start>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "288298c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "def compute_joint_proba_markov_assumption(text, word_counts, bigram_counts, threegram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "\n",
    "    for ngram in ngrammer(['<start>', '<start>'] + tokens + ['<end>']):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + \" \" + word2\n",
    "        if bigram in bigram_counts and ngram in threegram_counts:\n",
    "            prob += np.log(threegram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "\n",
    "    return prob, len(tokens)\n",
    "\n",
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d3d2f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news_perplex = Counter()\n",
    "bigrams_news_perplex = Counter()\n",
    "threegrams_news_perplex = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news_perplex.update(sentence)\n",
    "    bigrams_news_perplex.update(ngrammer(sentence,2))\n",
    "    threegrams_news_perplex.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "349d15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news_perplex = normalize(news[5000000:])\n",
    "\n",
    "vocab_news_perplex = Counter(norm_news_perplex)\n",
    "\n",
    "probas_news_perplex = Counter({word:c/len(norm_news_perplex) for word, c in vocab_news_perplex.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3c088500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from statistics import fmean, median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b752cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4915.06976176523\n",
      "4452.460986516732\n",
      "4846.350957869431\n"
     ]
    }
   ],
   "source": [
    "texts_for_perplex = generate(matrix_news, id2word_news, bi2id_news).replace('<end>', '\\n').replace('<start> <start>', '').split('\\n')\n",
    "\n",
    "perplexes = []\n",
    "for text in texts_for_perplex:\n",
    "    perplexes.append(perplexity(*compute_joint_proba(text, probas_news_perplex)))\n",
    "\n",
    "print(fmean(perplexes))\n",
    "print(median(perplexes))\n",
    "print(mode(perplexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "a5d077a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_beam_search(matrix, id2word, word2id, n=100, max_beams=5, start='<start> <start>'):\n",
    "\n",
    "    initial_node = Beam(sequence=start.split(), score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        new_beams = []\n",
    "\n",
    "        for beam in beams:\n",
    "\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "\n",
    "            last_id = word2id[' '.join(beam.sequence[-2:])]\n",
    "\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "\n",
    "            for top_id in top_idxs:\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                new_sequence = beam.sequence + [id2word[top_id]]\n",
    "\n",
    "                new_score = (beam.score + np.log1p(probas[top_id])) / len(new_sequence)\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "    \n",
    "\n",
    "    sorted_sequences = sorted(beams, key=lambda x: x.score, reverse=True)\n",
    "    sorted_sequences = [\" \".join(beam.sequence) for beam in sorted_sequences]\n",
    "    return sorted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5aa02611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['как сообщает риа новости',\n",
      " 'об этом риа новости',\n",
      " 'об этом сообщает риа новости',\n",
      " 'об этом сообщает агентство риа новости',\n",
      " 'об этом сообщает итар-тасс со ссылкой на пресс-службу мчс россии сергей '\n",
      " 'шойгу']\n"
     ]
    }
   ],
   "source": [
    "generation = generate_with_beam_search(matrix_news, id2word_news, bi2id_news)\n",
    "\n",
    "res_gen = list()\n",
    "for i in generation:\n",
    "    n = len(i)\n",
    "    res_gen.append(i[16:n-6])\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(res_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "96afe79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['как сообщили риа новости',\n",
      " 'как сообщает риа новости',\n",
      " 'как передает риа новости',\n",
      " 'как сообщает агентство риа новости',\n",
      " 'как сообщает итар-тасс со ссылкой на пресс-службу мчс россии сергей шойгу']\n"
     ]
    }
   ],
   "source": [
    "generation = generate_with_beam_search(matrix_news, id2word_news, bi2id_news, start = '<start> как')\n",
    "\n",
    "res_gen = list()\n",
    "for i in generation:\n",
    "    n = len(i)\n",
    "    res_gen.append(i[8:n-6])\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(res_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "770d07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['вчера владельцы азс уже оказались закрыты',\n",
      " 'вчера владельцы азс уже вели консультации с мэрией города',\n",
      " 'вчера флойд прошел через багамы вырывая деревья срывая линии электропередач '\n",
      " 'и крыши домов заливая морской водой улицы',\n",
      " 'вчера владельцы азс уже вели консультации с представителями российского '\n",
      " 'руководства',\n",
      " 'вчера флойд прошел через багамы вырывая деревья срывая линии электропередач']\n"
     ]
    }
   ],
   "source": [
    "generation = generate_with_beam_search(matrix_news, id2word_news, bi2id_news, start = '<start> вчера')\n",
    "\n",
    "res_gen = list()\n",
    "for i in generation:\n",
    "    n = len(i)\n",
    "    res_gen.append(i[8:n-6])\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(res_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "ec4d6ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['на протяжении многих лет производители табачных изделий несовершеннолетним',\n",
      " 'на протяжении многих лет импортировать в ливан эту продукцию',\n",
      " 'на протяжении двух предыдущихнедель и повлияли на результаты выборов будут '\n",
      " 'вне всякого сомнения оспорены теми кто будет ими недоволен',\n",
      " 'на протяжении многих лет импортировать в ливан в связи с этим в одиночку',\n",
      " 'на протяжении многих лет импортировать в ливан в связи с этим в одиночку '\n",
      " 'подчеркнул виторино в интервью риа новости']\n"
     ]
    }
   ],
   "source": [
    "generation = generate_with_beam_search(matrix_news, id2word_news, bi2id_news, start = '<start> на протяжении')\n",
    "\n",
    "res_gen = list()\n",
    "for i in generation:\n",
    "    n = len(i)\n",
    "    res_gen.append(i[8:n-6])\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(res_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
