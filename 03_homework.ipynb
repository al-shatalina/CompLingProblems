{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1 (5 балла)\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd8b4a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18f18432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embed = model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1f5a8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stemmer(sentence):\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    stemmed_sentence = ' '.join([ps.stem(token) for token in tokenized_sentence if token not in punctuation])\n",
    "    return stemmed_sentence\n",
    "\n",
    "def definitions(word):\n",
    "    defs = [d.definition() for d in wn.synsets(word)]\n",
    "    return defs\n",
    "\n",
    "def auto_context(i, sent):\n",
    "    res_context = list()\n",
    "    for j, word in enumerate(sent):\n",
    "        if j == i:\n",
    "            res_context.append(\"_\")\n",
    "        else:\n",
    "            res_context.append(ps.stem(word[1]))\n",
    "    return res_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "646cbb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(j_definition, j_context):\n",
    "    intersection = (set(j_definition) & set(j_context))\n",
    "    union = (set(j_definition) | set(j_context))\n",
    "    j_res =  len(intersection) / len(union)    \n",
    "    return (j_res)\n",
    "\n",
    "def result_w_jaccard (word, context):\n",
    "    res_def = list()\n",
    "    res_score = list()\n",
    "    max_index = 0\n",
    "    for definition in definitions(word):\n",
    "        res_def.append(definition)\n",
    "        res_score.append (jaccard(stemmer(definition), context))\n",
    "    max_score = max(res_score)\n",
    "    for i, score in enumerate(res_score):\n",
    "        if score == max_score:     \n",
    "            max_index = i\n",
    "    return res_def[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b6b8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(c_definition, c_context):\n",
    "    emb_context = embed(\" \".join(c_context))\n",
    "    emb_def = embed(\" \".join(c_definition))\n",
    "    c_res = cosine_similarity(emb_context.reshape(1,-1), emb_def.reshape(1, -1))\n",
    "    return (c_res)\n",
    "\n",
    "def result_w_cosine (word, context):\n",
    "    res_def = list()\n",
    "    res_score = list()\n",
    "    max_index = 0\n",
    "    for definition in definitions(word):\n",
    "        res_def.append(definition)\n",
    "        res_score.append (cosine(stemmer(definition), context))\n",
    "    max_score = max(res_score)\n",
    "    for i, score in enumerate(res_score):\n",
    "        if score == max_score:     \n",
    "            max_index = i\n",
    "    return res_def[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9a3c5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (results):\n",
    "    jac_accuracy = 0\n",
    "    cos_accuracy = 0\n",
    "    length = len(results)\n",
    "    for result in results:\n",
    "        target, jac_res, cos_res = result\n",
    "        if jac_res == target:\n",
    "            jac_accuracy+=1\n",
    "        if cos_res == target: \n",
    "            cos_accuracy+=1\n",
    "    jac_accuracy = (jac_accuracy*100)/length\n",
    "    cos_accuracy = (cos_accuracy*100)/length\n",
    "    print (\"jaccard: \", round(jac_accuracy), \"%\\ncosine: \", round(cos_accuracy), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9f4bb200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an unfortunate mishap; especially one causing damage or injury an unfortunate mishap; especially one causing damage or injury\n"
     ]
    }
   ],
   "source": [
    "context = 'The sign was damaged in an _.'\n",
    "stemmed_con = stemmer(context)\n",
    "result = result_w_jaccard ('accident', stemmed_con)\n",
    "result2 = result_w_cosine ('accident', stemmed_con)\n",
    "\n",
    "print(result, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6cf95052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard:  18 %\n",
      "cosine:  29 %\n"
     ]
    }
   ],
   "source": [
    "ress = list()\n",
    "for sentence in corpus_wsd[:40]:\n",
    "    for num, word in enumerate (sentence):\n",
    "        if word[0] == '':\n",
    "            continue\n",
    "        else:\n",
    "            target = wn.lemma_from_key(word[0]).synset().definition()\n",
    "            context = auto_context(num, sentence)\n",
    "            jac_res = result_w_jaccard(word[1], context)\n",
    "            cos_res = result_w_cosine(word[1], context)\n",
    "            ress.append ((target, jac_res, cos_res))\n",
    "accuracy(ress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2 (5 балла)\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, AgglomerativeClustering, OPTICS\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f91500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1f440",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a193ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001053019960000099\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]\n",
    "\n",
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "97f33bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0021290615824144776\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = DBSCAN(min_samples=3, eps=0.1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "39a981b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011271692824715207\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = DBSCAN(min_samples=5, eps=0.5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "437a8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = DBSCAN(min_samples=1, eps=1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9c3e0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011271692824715207\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = DBSCAN(min_samples=5, eps=0.5, algorithm='ball_tree')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b645c1",
   "metadata": {},
   "source": [
    "# AFFINITY PROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "84e55eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042740969848549505\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AffinityPropagation(damping=0.5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8d108c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042740969848549505\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AffinityPropagation(damping=0.6)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6aacec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04916074877739414\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AffinityPropagation(damping=0.9, convergence_iter=5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8551a1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04154515818974152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AffinityPropagation(damping=0.7, max_iter=100)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fa4d5e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04154515818974152\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AffinityPropagation(damping=0.8)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7881e2",
   "metadata": {},
   "source": [
    "# AGGLOMERATIVE CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b47f05a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011976265536517934\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AgglomerativeClustering()\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "080f492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030178341081673436\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AgglomerativeClustering(metric='l1', linkage='average')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a07f2db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030178341081673436\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AgglomerativeClustering(metric='l2', linkage='average')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1d16d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030178341081673436\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AgglomerativeClustering(metric='manhattan', linkage='average')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d21d3451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030178341081673436\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = AgglomerativeClustering(metric='cosine', linkage='average')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ddc99",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b137fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05765905738505665\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = KMeans(3)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ac6471af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06389873564638945\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = KMeans(3, algorithm='elkan')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "053a830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05702711328872276\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = KMeans(5)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a8e0172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06956126092867393\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = KMeans(5, algorithm='elkan')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ab2e3f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = KMeans(1)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8772c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea9486",
   "metadata": {},
   "source": [
    "# OPTICS\n",
    "\n",
    "Похож на DBSCAN.\n",
    "\n",
    "В отличие от DBSCAN, сохраняет иерархию кластеров для переменного радиуса окрестности. Лучше подходит для использования с большими наборами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4940eb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_001.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_001.png\",\n",
    "     width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a6a9474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008342972897728207\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = OPTICS(min_samples=6, eps=2)\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a43f664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0022488775628999045\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = OPTICS(min_samples=6, eps=2, metric = 'cityblock')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "793fead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00883068382727673\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = OPTICS(min_samples=6, eps=2, metric = 'cosine')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f45ec5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\cluster\\_optics.py:333: DataConversionWarning: Data will be converted to boolean for metric jaccard, to avoid this warning, you may convert the data prior to calling fit.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\cluster\\_optics.py:333: DataConversionWarning: Data will be converted to boolean for metric jaccard, to avoid this warning, you may convert the data prior to calling fit.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\cluster\\_optics.py:333: DataConversionWarning: Data will be converted to boolean for metric jaccard, to avoid this warning, you may convert the data prior to calling fit.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\scoop\\apps\\python\\current\\Lib\\site-packages\\sklearn\\cluster\\_optics.py:333: DataConversionWarning: Data will be converted to boolean for metric jaccard, to avoid this warning, you may convert the data prior to calling fit.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = OPTICS(min_samples=6, eps=2, metric = 'jaccard')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9072a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00883068382727673\n"
     ]
    }
   ],
   "source": [
    "ARI = []\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "    texts = grouped_df.get_group(key)['context'].values\n",
    "\n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    cluster = OPTICS(min_samples=6, eps=2, metric = 'cosine')\n",
    "    \n",
    "    cluster.fit(X)\n",
    "    labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "    \n",
    "print(np.mean(ARI)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
