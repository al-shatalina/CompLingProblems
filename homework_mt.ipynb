{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на том же корпусе но в другую сторону - с русского на английский.\n",
        "Можно использовать как основу первый или второй способ реализации (с MultiheadAttention или с nn.Transformer). Подберите несколько тестовых примеров для проверки обучения на каждой эпохе.\n",
        "\n",
        "Параметры ниже точно работают в колабе и модель обучается достаточно быстро. Попробуйте их немного увеличить (batch size возможно придется наоборот уменьшить). Обучайте модель хотя бы 5 эпох, а желательно больше, чтобы тестовые примеры начали переводиться более менее адекватно.\n",
        "\n",
        "После обучения возьмите хотя бы 100 примером из тестовой части параллельного корпуса и переведите их. Оцените качество переводов с помощью метрики BLEU (пример использования ниже)\n",
        "Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [EOS] в текущем коде не сработает).\n",
        "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! Функция с batch prediction должна работать быстрее, поэтому переведите всю тестовую выборку и оцените качество BLEU на всех данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05d202c4",
      "metadata": {
        "id": "05d202c4",
        "outputId": "29676aa7-7c77-4981-dedf-868cf3a06d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.5.0)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.6.0+cu124)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.22.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# В колабе установите torchtune и torchao, чтобы семинарская тетрадка работала\n",
        "!pip install torchtune torchao"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "from torch.nn import Transformer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M0FHb4CTAyha"
      },
      "id": "M0FHb4CTAyha",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLKhB_4XA1wh",
        "outputId": "07645519-189a-4aeb-8c0e-82d2f1247ead"
      },
      "id": "dLKhB_4XA1wh",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-31 05:12:58--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  24.6MB/s    in 5.8s    \n",
            "\n",
            "2025-03-31 05:13:05 (19.8 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
            "\n",
            "--2025-03-31 05:13:05--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  18.7MB/s    in 3.6s    \n",
            "\n",
            "2025-03-31 05:13:09 (17.8 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
            "\n",
            "--2025-03-31 05:13:09--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305669 (299K)\n",
            "Saving to: ‘opus.en-ru-test.ru’\n",
            "\n",
            "opus.en-ru-test.ru  100%[===================>] 298.50K   534KB/s    in 0.6s    \n",
            "\n",
            "2025-03-31 05:13:10 (534 KB/s) - ‘opus.en-ru-test.ru’ saved [305669/305669]\n",
            "\n",
            "--2025-03-31 05:13:10--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173307 (169K)\n",
            "Saving to: ‘opus.en-ru-test.en’\n",
            "\n",
            "opus.en-ru-test.en  100%[===================>] 169.25K   306KB/s    in 0.6s    \n",
            "\n",
            "2025-03-31 05:13:11 (306 KB/s) - ‘opus.en-ru-test.en’ saved [173307/173307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "5Hxt0paUBGIw"
      },
      "id": "5Hxt0paUBGIw",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().splitlines()"
      ],
      "metadata": {
        "id": "eEI8pSRVBHqB"
      },
      "id": "eEI8pSRVBHqB",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_ru = Tokenizer(BPE())\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)\n",
        "\n",
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)"
      ],
      "metadata": {
        "id": "g8DCiGYFBJwI"
      },
      "id": "g8DCiGYFBJwI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!!\n",
        "tokenizer_en.decoder = decoders.BPEDecoder()\n",
        "tokenizer_ru.decoder = decoders.BPEDecoder()"
      ],
      "metadata": {
        "id": "pfwQod1ZBjIx"
      },
      "id": "pfwQod1ZBjIx",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')"
      ],
      "metadata": {
        "id": "3nxsqV0VBnHA"
      },
      "id": "3nxsqV0VBnHA",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "metadata": {
        "id": "UOdQBRRABo9o"
      },
      "id": "UOdQBRRABo9o",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len, encoder=False):\n",
        "    if encoder:\n",
        "        return tokenizer.encode(text).ids[:max_len]\n",
        "    else:\n",
        "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
      ],
      "metadata": {
        "id": "e7ClnI8ZBqIh"
      },
      "id": "e7ClnI8ZBqIh",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyLsyX1HBxM5",
        "outputId": "a7df72d4-3f68-4f51-f514-6511957fd666"
      },
      "id": "zyLsyX1HBxM5",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_ru = 47, 48"
      ],
      "metadata": {
        "id": "nxJJPbY8B0oY"
      },
      "id": "nxJJPbY8B0oY",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru, encoder=True) for t in ru_sents]"
      ],
      "metadata": {
        "id": "Q5UdrFAtB2Kh"
      },
      "id": "Q5UdrFAtB2Kh",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_ru, texts_en):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_ru)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[index]\n",
        "        ids_ru = self.texts_ru[index]\n",
        "\n",
        "        return ids_ru, ids_en"
      ],
      "metadata": {
        "id": "2g7gQhFTCHCx"
      },
      "id": "2g7gQhFTCHCx",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ru_train, X_ru_valid, X_en_train, X_en_valid, = train_test_split(X_ru, X_en, test_size=0.05)"
      ],
      "metadata": {
        "id": "q5SOJTfZCJco"
      },
      "id": "q5SOJTfZCJco",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
        "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
        "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "\n",
        "        src_embedded = self.embedding_enc(src)\n",
        "        B,S,E = src_embedded.shape\n",
        "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "        tgt_embedded = self.embedding_dec(tgt)\n",
        "        B,S,E = tgt_embedded.shape\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "\n",
        "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
        "\n",
        "        encoder_output = self.transformer.encoder(\n",
        "            src_embedded,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        decoder_output = self.transformer.decoder(\n",
        "            tgt_embedded,\n",
        "            encoder_output,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "TKCpQsErCL6x"
      },
      "id": "TKCpQsErCL6x",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
        "vocab_size_enc = tokenizer_ru.get_vocab_size()\n",
        "vocab_size_dec = tokenizer_en.get_vocab_size()\n",
        "\n",
        "embed_dim = 36\n",
        "num_heads = 6\n",
        "ff_dim = embed_dim*2\n",
        "num_layers = 4\n",
        "batch_size = 200\n",
        "\n",
        "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
      ],
      "metadata": {
        "id": "Tspkiz-4CT0w"
      },
      "id": "Tspkiz-4CT0w",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(X_ru_train, X_en_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
        "\n",
        "valid_set = Dataset(X_ru_valid, X_en_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "D3abf4L9CXjJ"
      },
      "id": "D3abf4L9CXjJ",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=100):\n",
        "\n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "        texts_en = texts_en.to(DEVICE)\n",
        "        texts_ru = texts_ru.to(DEVICE)\n",
        "        texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "        texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "        src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "        tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "        logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        B,S,C = logits.shape\n",
        "        loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        if run is not None:\n",
        "            run.log({\"loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, run=None):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "            texts_en = texts_en.to(DEVICE)\n",
        "            texts_ru = texts_ru.to(DEVICE)\n",
        "            texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "            texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "            src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "            tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "            logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "            B,S,C = logits.shape\n",
        "            loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "            epoch_loss.append(loss.item())\n",
        "            if run is not None:\n",
        "                run.log({\"val_loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "yt6m2ZNsH2Gx"
      },
      "id": "yt6m2ZNsH2Gx",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad\n",
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = tokenizer_ru.encode(text).ids[:max_len_ru]\n",
        "    output_ids = [tokenizer_en.token_to_id('[BOS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "\n",
        "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
        "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_en.token_to_id('[EOS]'), tokenizer_en.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "        pred = logits.argmax(2).view(-1)[-1].item()\n",
        "\n",
        "    return tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[1:]])"
      ],
      "metadata": {
        "id": "VJzJrjb4HzeR"
      },
      "id": "VJzJrjb4HzeR",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_model = torch.nn.Linear(2, 1)\n",
        "fake_optimizer = torch.optim.AdamW(fake_model.parameters(), lr=0.0001)\n",
        "fake_scheduler = torch.optim.lr_scheduler.OneCycleLR(fake_optimizer, max_lr=0.001, pct_start=0.05,\n",
        "                                                steps_per_epoch=50, epochs=20)\n",
        "lrs = []\n",
        "\n",
        "for i in range(1000):\n",
        "    fake_optimizer.step()\n",
        "    lrs.append(fake_optimizer.param_groups[0][\"lr\"])\n",
        "    fake_scheduler.step()\n",
        "\n",
        "plt.plot(lrs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "9CdG0i9_CY2Q",
        "outputId": "48bc7a60-3494-4c4c-cefa-92b34b4d4ffb"
      },
      "id": "9CdG0i9_CY2Q",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x784a69be8b90>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "mK3w-o_iHks5"
      },
      "id": "mK3w-o_iHks5",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, pct_start=0.05,\n",
        "                                                steps_per_epoch=len(training_generator), epochs=5)"
      ],
      "metadata": {
        "id": "u9KxQIp8CaFh"
      },
      "id": "u9KxQIp8CaFh",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "losses = []\n",
        "run = None\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
        "\n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "    print(translate(\"Это пример\"))\n",
        "    print(translate('Ты можешь это перевести?'))\n",
        "    print(translate('Что ты собираешься с этим делать?'))\n",
        "    print(translate('Жизнь прекрасна'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vbl9eZeCdK5",
        "outputId": "e67f731f-ad88-4220-e19c-9e4d38a44379"
      },
      "id": "4Vbl9eZeCdK5",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 10.27358736038208;\n",
            "Loss: 10.115881233215331;\n",
            "Loss: 9.898197189966838;\n",
            "Loss: 9.584483551979066;\n",
            "Loss: 9.186367567062378;\n",
            "Loss: 8.81345179160436;\n",
            "Loss: 8.515476082393102;\n",
            "Loss: 8.269112652540207;\n",
            "Loss: 8.056223652627732;\n",
            "Loss: 7.872862775802612;\n",
            "Loss: 7.711205448237332;\n",
            "Loss: 7.566784700155258;\n",
            "Loss: 7.437860993972191;\n",
            "Loss: 7.3228903497968405;\n",
            "Loss: 7.217647878646851;\n",
            "Loss: 7.121555885076523;\n",
            "Loss: 7.036190709226272;\n",
            "Loss: 6.957503888871934;\n",
            "Loss: 6.88446586257533;\n",
            "Loss: 6.816735849142074;\n",
            "Loss: 6.753394083295549;\n",
            "Loss: 6.693670038960197;\n",
            "Loss: 6.638633356923642;\n",
            "Loss: 6.587284903128942;\n",
            "Loss: 6.539159935379028;\n",
            "Loss: 6.493973179046924;\n",
            "Loss: 6.4504225552523575;\n",
            "Loss: 6.409169994422368;\n",
            "Loss: 6.370332542123466;\n",
            "Loss: 6.332695086956024;\n",
            "Loss: 6.297770671229209;\n",
            "Loss: 6.264064729064703;\n",
            "Loss: 6.231535665483186;\n",
            "Loss: 6.200319779620451;\n",
            "Loss: 6.170404753548758;\n",
            "Loss: 6.141634365055296;\n",
            "Loss: 6.114014082599331;\n",
            "Loss: 6.08771583343807;\n",
            "Loss: 6.0616450709563034;\n",
            "Loss: 6.036793909430504;\n",
            "Loss: 6.013173662162409;\n",
            "Loss: 5.990203450293768;\n",
            "Loss: 5.967785279140916;\n",
            "Loss: 5.946316790255633;\n",
            "Loss: 5.925330921914842;\n",
            "Loss: 5.9050500971338025;\n",
            "Loss: 5.885655314161422;\n",
            "First epoch - 4.8861942386627195, saving model..\n",
            "Epoch: 1, Train loss: 5.876, Val loss: 4.886,            Epoch time=548.164s\n",
            "It ' s a little good .\n",
            "You ' re going to do you ?\n",
            "What are you doing ?\n",
            "The following of the\n",
            "Loss: 4.942659420967102;\n",
            "Loss: 4.929869260787964;\n",
            "Loss: 4.928638822237651;\n",
            "Loss: 4.923947147130966;\n",
            "Loss: 4.9227775363922115;\n",
            "Loss: 4.920535283088684;\n",
            "Loss: 4.917437423297337;\n",
            "Loss: 4.916064850687981;\n",
            "Loss: 4.913502682579888;\n",
            "Loss: 4.908769072055817;\n",
            "Loss: 4.903458962873979;\n",
            "Loss: 4.8991362961133325;\n",
            "Loss: 4.893427538504968;\n",
            "Loss: 4.89048463344574;\n",
            "Loss: 4.887406423250834;\n",
            "Loss: 4.883663699030876;\n",
            "Loss: 4.880401562522439;\n",
            "Loss: 4.876611086262597;\n",
            "Loss: 4.873183833423414;\n",
            "Loss: 4.868858412265777;\n",
            "Loss: 4.865041133108593;\n",
            "Loss: 4.862026440880515;\n",
            "Loss: 4.858270506858826;\n",
            "Loss: 4.854414073824882;\n",
            "Loss: 4.851177474403381;\n",
            "Loss: 4.84690078827051;\n",
            "Loss: 4.844125382811935;\n",
            "Loss: 4.840526065656117;\n",
            "Loss: 4.83702776497808;\n",
            "Loss: 4.833921008904775;\n",
            "Loss: 4.830647951556791;\n",
            "Loss: 4.82740889057517;\n",
            "Loss: 4.824496892582286;\n",
            "Loss: 4.820511998008279;\n",
            "Loss: 4.81736118207659;\n",
            "Loss: 4.814310796923108;\n",
            "Loss: 4.81100262989869;\n",
            "Loss: 4.808376250768963;\n",
            "Loss: 4.805708454449971;\n",
            "Loss: 4.802720869421959;\n",
            "Loss: 4.80018354113509;\n",
            "Loss: 4.79708342677071;\n",
            "Loss: 4.7943145506881;\n",
            "Loss: 4.79139798857949;\n",
            "Loss: 4.78827805879381;\n",
            "Loss: 4.785366133710612;\n",
            "Loss: 4.782738977594579;\n",
            "Improved from 4.8861942386627195 to 4.566357410430908, saving model..\n",
            "Epoch: 2, Train loss: 4.781, Val loss: 4.566,            Epoch time=547.306s\n",
            "It ' s a little bit .\n",
            "You want to know that ?\n",
            "What are you doing ?\n",
            "\n",
            "Loss: 4.63480429649353;\n",
            "Loss: 4.626077375411987;\n",
            "Loss: 4.62830945332845;\n",
            "Loss: 4.629759274721145;\n",
            "Loss: 4.62757706451416;\n",
            "Loss: 4.6221756482124325;\n",
            "Loss: 4.6192745726449145;\n",
            "Loss: 4.619046067595482;\n",
            "Loss: 4.617577508820427;\n",
            "Loss: 4.615265012741089;\n",
            "Loss: 4.613081275333058;\n",
            "Loss: 4.611359567244848;\n",
            "Loss: 4.6096661798770615;\n",
            "Loss: 4.607748137882778;\n",
            "Loss: 4.60568890953064;\n",
            "Loss: 4.605393805205822;\n",
            "Loss: 4.6051794722500965;\n",
            "Loss: 4.60384156677458;\n",
            "Loss: 4.603320468852394;\n",
            "Loss: 4.601937346696854;\n",
            "Loss: 4.60053220476423;\n",
            "Loss: 4.599026506380602;\n",
            "Loss: 4.5975552094501;\n",
            "Loss: 4.596111135085423;\n",
            "Loss: 4.594495624351501;\n",
            "Loss: 4.592605214118958;\n",
            "Loss: 4.59125376118554;\n",
            "Loss: 4.590052785021919;\n",
            "Loss: 4.588856420845821;\n",
            "Loss: 4.587039249738058;\n",
            "Loss: 4.585711526255454;\n",
            "Loss: 4.584477798193693;\n",
            "Loss: 4.582744683930368;\n",
            "Loss: 4.581583040602067;\n",
            "Loss: 4.58036669077192;\n",
            "Loss: 4.579359180132548;\n",
            "Loss: 4.577942018895536;\n",
            "Loss: 4.5771337313401075;\n",
            "Loss: 4.575711010297139;\n",
            "Loss: 4.574499235272407;\n",
            "Loss: 4.5732835913867484;\n",
            "Loss: 4.571736454963684;\n",
            "Loss: 4.570249335710392;\n",
            "Loss: 4.569426545663314;\n",
            "Loss: 4.568108064651489;\n",
            "Loss: 4.566560524442922;\n",
            "Loss: 4.564960491707985;\n",
            "Improved from 4.566357410430908 to 4.427446641921997, saving model..\n",
            "Epoch: 3, Train loss: 4.564, Val loss: 4.427,            Epoch time=546.268s\n",
            "That ' s the way of the world .\n",
            "You can ' t do that ?\n",
            "What are you doing ?\n",
            "\n",
            "Loss: 4.503930687904358;\n",
            "Loss: 4.4943783926963805;\n",
            "Loss: 4.486554385821025;\n",
            "Loss: 4.488592734336853;\n",
            "Loss: 4.489110686302185;\n",
            "Loss: 4.484070281982422;\n",
            "Loss: 4.4857075445992605;\n",
            "Loss: 4.483865786790847;\n",
            "Loss: 4.483215177853902;\n",
            "Loss: 4.482966534137725;\n",
            "Loss: 4.48089875611392;\n",
            "Loss: 4.48072162270546;\n",
            "Loss: 4.47965494962839;\n",
            "Loss: 4.479760382856641;\n",
            "Loss: 4.4789330739974975;\n",
            "Loss: 4.478833998143673;\n",
            "Loss: 4.4780679402631876;\n",
            "Loss: 4.477575126753913;\n",
            "Loss: 4.476609444367258;\n",
            "Loss: 4.476741338729858;\n",
            "Loss: 4.475777203241984;\n",
            "Loss: 4.475221125862815;\n",
            "Loss: 4.474763464098391;\n",
            "Loss: 4.47510458111763;\n",
            "Loss: 4.474989438438415;\n",
            "Loss: 4.474091164148771;\n",
            "Loss: 4.473244063942521;\n",
            "Loss: 4.472848810979298;\n",
            "Loss: 4.472480363188119;\n",
            "Loss: 4.4720632359186805;\n",
            "Loss: 4.470786579039789;\n",
            "Loss: 4.469619683921337;\n",
            "Loss: 4.469177235979022;\n",
            "Loss: 4.468279512349297;\n",
            "Loss: 4.467397674288068;\n",
            "Loss: 4.4668370424376596;\n",
            "Loss: 4.46657203609879;\n",
            "Loss: 4.466256326499738;\n",
            "Loss: 4.465599514154287;\n",
            "Loss: 4.465022768974304;\n",
            "Loss: 4.464799424962299;\n",
            "Loss: 4.464739542688642;\n",
            "Loss: 4.464239278172338;\n",
            "Loss: 4.463812874989076;\n",
            "Loss: 4.462938324928284;\n",
            "Loss: 4.462446124864661;\n",
            "Loss: 4.46184668743864;\n",
            "Improved from 4.427446641921997 to 4.370439352035523, saving model..\n",
            "Epoch: 4, Train loss: 4.462, Val loss: 4.370,            Epoch time=547.337s\n",
            "It ' s a lot of the world .\n",
            "You can ' t be a message ?\n",
            "What are you doing ?\n",
            "\n",
            "Loss: 4.421486897468567;\n",
            "Loss: 4.419107294082641;\n",
            "Loss: 4.4203912512461345;\n",
            "Loss: 4.423567031621933;\n",
            "Loss: 4.422485060691834;\n",
            "Loss: 4.4231182726224265;\n",
            "Loss: 4.426170557567051;\n",
            "Loss: 4.427972793579102;\n",
            "Loss: 4.426088405185276;\n",
            "Loss: 4.425422337532043;\n",
            "Loss: 4.425770108916542;\n",
            "Loss: 4.427001936435699;\n",
            "Loss: 4.425995799578153;\n",
            "Loss: 4.425717096328736;\n",
            "Loss: 4.425328238805135;\n",
            "Loss: 4.425169760882855;\n",
            "Loss: 4.424503027972053;\n",
            "Loss: 4.424964235093858;\n",
            "Loss: 4.424856838678059;\n",
            "Loss: 4.425377213478089;\n",
            "Loss: 4.42447510151636;\n",
            "Loss: 4.424398746923967;\n",
            "Loss: 4.4243668010960455;\n",
            "Loss: 4.424552532831828;\n",
            "Loss: 4.424455333328247;\n",
            "Loss: 4.424318963564359;\n",
            "Loss: 4.423774754382946;\n",
            "Loss: 4.423888190644128;\n",
            "Loss: 4.423594879610785;\n",
            "Loss: 4.422998037815094;\n",
            "Loss: 4.423429380386106;\n",
            "Loss: 4.423519246727228;\n",
            "Loss: 4.422741689248518;\n",
            "Loss: 4.423288637750289;\n",
            "Loss: 4.422874565941947;\n",
            "Loss: 4.422767290141848;\n",
            "Loss: 4.423253856865135;\n",
            "Loss: 4.423372981799276;\n",
            "Loss: 4.423720710094159;\n",
            "Loss: 4.42348568880558;\n",
            "Loss: 4.423656675757432;\n",
            "Loss: 4.42355866647902;\n",
            "Loss: 4.423085764618807;\n",
            "Loss: 4.423011230988936;\n",
            "Loss: 4.423036543740166;\n",
            "Loss: 4.422760605397432;\n",
            "Loss: 4.422237330903398;\n",
            "Improved from 4.370439352035523 to 4.361415735244751, saving model..\n",
            "Epoch: 5, Train loss: 4.422, Val loss: 4.361,            Epoch time=546.696s\n",
            "It ' s a lot of the world .\n",
            "You can ' t be a message ?\n",
            "What are you doing with me ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU"
      ],
      "metadata": {
        "id": "nKDfAGhUAzih"
      },
      "id": "nKDfAGhUAzih"
    },
    {
      "cell_type": "code",
      "source": [
        "data_hypothesis = ru_sents[:150]\n",
        "data_reference = en_sents[:150]"
      ],
      "metadata": {
        "id": "XWaelvw8DFFI"
      },
      "id": "XWaelvw8DFFI",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses = []\n",
        "for data in data_hypothesis:\n",
        "  hypotheses.append(translate(data))"
      ],
      "metadata": {
        "id": "ED3Q2qojDnqx"
      },
      "id": "ED3Q2qojDnqx",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "BLEUscores = {}\n",
        "for i, hypothesis in enumerate(hypotheses):\n",
        "  reference = data_reference[i].split()\n",
        "  hypothesis_split = hypothesis.split()\n",
        "  ref_hyp = (data_reference[i], hypothesis)\n",
        "  BLEUscores[ref_hyp] = (nltk.translate.bleu_score.sentence_bleu([reference], hypothesis_split, auto_reweigh=True))"
      ],
      "metadata": {
        "id": "MEZHN6KlEO2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3593b0c-f582-4091-fa6d-799c0011a31b"
      },
      "id": "MEZHN6KlEO2x",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_5 = sorted(BLEUscores.items(), key=lambda item: item[1], reverse=True)[:5]"
      ],
      "metadata": {
        "id": "DyTDQMstBcTV"
      },
      "id": "DyTDQMstBcTV",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW2KAs3dCQTp",
        "outputId": "bfc98788-a4b3-4662-9828-4fdba7b66beb"
      },
      "id": "TW2KAs3dCQTp",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('!', '!'), 1.0),\n",
              " (('I need to talk to him.', 'I need to talk to him .'), 0.6147881529512643),\n",
              " (('The time now is 11:38 AM .', 'The time now is 04 : 00 AM .'),\n",
              "  0.3549481056010053),\n",
              " (('The time now is 02:25 PM .', 'The time now is 04 : 30 AM .'),\n",
              "  0.3155984539112945),\n",
              " ((\"It encourages the State party to make full use of the Committee's general recommendation 19 in such efforts and of the United Nations Secretary-General's in-depth study on all forms of violence against women (A/61/122/Add.1 and Corr.1).\",\n",
              "   'He urges the State party to ensure that the Secretary - General on the United Nations system and the United Nations system of the United Nations system , including the United Nations system of the United Nations system ( A / CN .'),\n",
              "  0.12273033502938982)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
        "\n",
        "Ответ должен содержать как минимум 10 предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "back translation - техника аугментации данных для машинного перевода. Она используется, когда у одного из двух языков участвующих в переводе больший объем данных, чем у другого.\n",
        "\n",
        "Для начала тренируется обратная модель, то есть если нам надо обучить en->ru, сначала мы обучаем ru->en. Эта обратная модель используется для перевода текстов на нашем target языке на язык source. То есть мы переводим русские тексты на английский и присоединяем эти синтетические тексты к трейн выборке. Затем мы переобучаем нашу модель уже в нужном направлении en->ru.\n",
        "\n",
        "back translation можно осуществлять разными способами ввиду разнообразия параметров. В частности, это разнообразие относится к тому, как мы генерируем \"обратные\" данные для промежуточной модели. Мы можем использовать \"жадный\" инференс, beam search, temperature sampling или upsampling.\n",
        "\n",
        "Таким образом нам понадобятся 2 модели и 2 запуска обучения.\n",
        "\n"
      ],
      "metadata": {
        "id": "jpJDA4mGDhyK"
      },
      "id": "jpJDA4mGDhyK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}